[General]
seed = 42
n_replicates = 1
n_folds = 5
log_level = INFO

# Pipeline will create this folder, if didn't exist
results_dir = pc_pred_results

n_prediction_repeats = 1 

[InputData]
train_geno_path = ../data/train_genotype.vcf
train_pheno_path = ../data/train_phenotype.csv

# If the variables test_geno_path and test_pheno_path is unavailable or commented out, the pipeline must run in cross-validation mode

test_geno_path = ../data/test_genotype.vcf
test_pheno_path = ../data/test_phenotype.csv

[FeatureView]
# Options: SNP | PC | HAP
# Default: SNP
feature_view = PC

[FeatureViewSettings]
# SNP:   Use all SNP markers directly as features.

# PCA:   Use Principal Components from SNPs.
# - You can specify either:
#   pca_components = 50              # Fixed number of PCs
#   OR
#   pca_variance_threshold = 0.95    # Keep enough PCs to explain 95% variance

# pca_components = 50
pca_variance_threshold = 0.95

rtm-gwas-snpldbtool = ../../omniGS/utils/rtm-gwas/rtm-gwas-snpldb

[Models]
# Classical GS
BRR = True

# Linear / Regression-based
ELASTICNET = False
LASSO = False

# Tree-based
RANDOMFOREST = True
XGBOOST = True
LIGHTGBM = True

# Neural Network Based
MLP = True
CNN = False

# Ensemble
STACKING = True

# Hyper-Parameters for all available Models
# =========================================
# Classical GS
# =========================================
[Hyperparameters_BRR]
max_iter = 300
tol = 0.001
alpha_1 = 1e-6
alpha_2 = 1e-6
lambda_1 = 1e-6
lambda_2 = 1e-6

# ================================
# Linear / Regression-based
# ================================
[Hyperparameters_ELASTICNET]
alpha = 1.0
l1_ratio = 0.5
max_iter = 1000
tol = 0.001

[Hyperparameters_LASSO]
alpha = 0.01
max_iter = 1000
tol = 0.001

# ================================
# Tree-based
# ================================
[Hyperparameters_RANDOMFOREST]
n_estimators = 200
max_depth = None
max_features = sqrt
min_samples_split = 2
min_samples_leaf = 1
n_jobs = 4

[Hyperparameters_XGBOOST]
n_estimators = 500
learning_rate = 0.05
max_depth = 6
subsample = 0.8
colsample_bytree = 0.8
reg_lambda = 1.0
reg_alpha = 0.0
n_jobs = 4

[Hyperparameters_LIGHTGBM]
n_estimators = 500
learning_rate = 0.05
max_depth = -1
num_leaves = 31
subsample = 0.8
colsample_bytree = 0.8
reg_lambda = 1.0
reg_alpha = 0.0
n_jobs = 4

# ================================
# Neural Networks
# ================================
[Hyperparameters_MLP]
hidden_layers = [512,256,128]
activation = relu
dropout = 0.5
lr = 0.0005
weight_decay = 0.0015
epochs = 100
batch_size = 32
device = cpu
patience = 10
grad_clip = 1.0
early_stopping = true
input_dropout = 0.05

[Hyperparameters_CNN]
conv_channels = [32,64]
kernel_sizes = [5,3]
hidden_layers = [128]
activation = relu
dropout = 0.3
lr = 0.001
weight_decay = 0.0001
epochs = 100
batch_size = 32
device = cpu
patience = 10
warmup_ratio = 0.1
grad_clip = 1.0
early_stopping = true

# ================================
# Ensemble
# ================================
[Hyperparameters_STACKING]
base_models = [BRR, MLP]
meta_model = BRR
cv = 3