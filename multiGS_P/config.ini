[General]
seed = 42
n_replicates = 10
n_folds = 5
log_level = INFO

# Pipeline will create this folder, if didn't exist
results_dir = /path to results

# Number of times prediction needs to be evaluated.
n_prediction_repeats = 10 

[InputData]
train_geno_path = /path to Genotypes for Training Population
train_pheno_path = /path to Phenotypes for Training Population

# If the variables test_geno_path and test_pheno_path is unavailable or commented out, the pipeline must run in cross-validation mode
test_geno_path = /path to Genotypes for Testing Population
test_pheno_path = /path to Phenotypes for Testing Population


[FeatureView]
# Options: SNP | PC | HAP
# Default: SNP

feature_view = SNP

[FeatureViewSettings]
# SNP:   Use all SNP markers directly as features.

# PCA:   Use Principal Components from SNPs.
# - You can specify either:
#   pca_components = 50              # Fixed number of PCs
#   OR
#   pca_variance_threshold = 0.95    # Keep enough PCs to explain 95% variance

# pca_components = 50
pca_variance_threshold = 0.95

# HAP: Calculate the halpotypes blocks using SNP VCF
rtm-gwas-snpldbtool = /path to rtm-gwas tool

[Models]
# Classical GS
BRR = True

# Linear / Regression-based
ELASTICNET = True
LASSO = True

# Tree-based
RANDOMFOREST = True
XGBOOST = True
LIGHTGBM = True

# Neural Network Based
MLP = True

# Ensemble
STACKING = True

# Hyper-Parameters for all available Models
# =========================================
# Classical GS
# =========================================
[Hyperparameters_GBLUP]
alpha = 1.0
max_iter = 1000

[Hyperparameters_BRR]
max_iter = 300
alpha_1 = 1e-6
alpha_2 = 1e-6
lambda_1 = 1e-6
lambda_2 = 1e-6

# ================================
# Linear / Regression-based
# ================================
[Hyperparameters_ELASTICNET]
alpha = 1.0
l1_ratio = 0.5
max_iter = 1000


[Hyperparameters_LASSO]
alpha = 0.01
max_iter = 1000

# ================================
# Tree-based
# ================================
[Hyperparameters_RANDOMFOREST]
n_estimators = 200
max_depth = None
max_features = sqrt
min_samples_split = 2
min_samples_leaf = 1
n_jobs = 4

[Hyperparameters_XGBOOST]
n_estimators = 500
learning_rate = 0.05
max_depth = 6
subsample = 0.8
colsample_bytree = 0.8
reg_lambda = 1.0
reg_alpha = 0.0
n_jobs = 4

[Hyperparameters_LIGHTGBM]
n_estimators = 500
learning_rate = 0.05
max_depth = -1
num_leaves = 31
subsample = 0.8
colsample_bytree = 0.8
reg_lambda = 1.0
reg_alpha = 0.0
n_jobs = 4

# ================================
# Neural Networks
# ================================
[Hyperparameters_MLP]
hidden_layers = [512,256,128]   
activation = relu          
dropout = 0.5              
lr = 0.0005                 
weight_decay = 0.0015      
epochs = 100               
batch_size = 32            
device = cpu               
patience = 10              
warmup_ratio = 0.1         
grad_clip = 1.0            
early_stopping = true      

# ================================
# Ensemble
# ================================
[Hyperparameters_STACKING]
base_models = [BRR, MLP]
meta_model = BRR
cv = 5